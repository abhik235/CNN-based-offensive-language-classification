{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-YibpL3v8Sf8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow' # Why theano why not\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'callback'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e2ab98355e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'callback'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to calculate f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.compile(loss=\\'binary_crossentropy\\',\\n          optimizer= \"adam\",\\n          metrics=[f1])'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model.compile(loss='binary_crossentropy',\n",
    "          optimizer= \"adam\",\n",
    "          metrics=[f1])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook_corpus_msr_1723796</td>\n",
       "      <td>Well said sonu..you have courage to stand agai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook_corpus_msr_466073</td>\n",
       "      <td>Most of Private Banks ATM's Like HDFC, ICICI e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook_corpus_msr_1493901</td>\n",
       "      <td>Now question is, Pakistan will adhere to this?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook_corpus_msr_405512</td>\n",
       "      <td>Pakistan is comprised of fake muslims who does...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0  \\\n",
       "0  facebook_corpus_msr_1723796   \n",
       "1   facebook_corpus_msr_466073   \n",
       "2  facebook_corpus_msr_1493901   \n",
       "3   facebook_corpus_msr_405512   \n",
       "\n",
       "                                                   1  2  \n",
       "0  Well said sonu..you have courage to stand agai...  1  \n",
       "1  Most of Private Banks ATM's Like HDFC, ICICI e...  0  \n",
       "2     Now question is, Pakistan will adhere to this?  1  \n",
       "3  Pakistan is comprised of fake muslims who does...  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/mirafra/Desktop/nlp_project/english/agr_en_train.csv',header = None)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 message  class\n",
      "0      Well said sonu..you have courage to stand agai...      1\n",
      "1      Most of Private Banks ATM's Like HDFC, ICICI e...      0\n",
      "2         Now question is, Pakistan will adhere to this?      1\n",
      "3      Pakistan is comprised of fake muslims who does...      1\n",
      "4      ??we r against cow slaughter,so of course it w...      0\n",
      "5      Wondering why Educated Ambassador is strugglin...      0\n",
      "6      How does inflation react to all the after shoc...      0\n",
      "7      Not good job.....this guis creating a problem ...      0\n",
      "8      This is a false news Indian media is simply mi...      0\n",
      "9      no permanent foes, no permanent friends. inter...      0\n",
      "10     Deepak Kumar Sharma Saab...chalo aap ki Ye baa...      0\n",
      "11     Communist parties killed lacks of opponents in...      1\n",
      "12     Why you guys counter the modi govt decisions, ...      0\n",
      "13              Rss is 3 time ban terrorist organization      1\n",
      "14     No \\nSame acting ll be there .. \\nbut we ll wa...      0\n",
      "15     Happy Diwali.!!let's wish the next one year he...      0\n",
      "16     Lolz... He said he is gonna employ large numbe...      0\n",
      "17                               So funny stupid,,,,,,,,      1\n",
      "18     absolutely! the deeper you dive the shallower ...      0\n",
      "19     Brown Sahib , anti national leftist commies , ...      0\n",
      "20     Now nifty above 20 day moving average, what ne...      0\n",
      "21     Good to see when their in no terrror in pak an...      0\n",
      "22     First of all,there is no gaurakshak in assam.t...      0\n",
      "23                     AAP dont need the monsters like u      1\n",
      "24     For tht those hv more thn 2 flats or 2 land is...      0\n",
      "25     Once upon a time Bengal/ Kolkata was most pros...      0\n",
      "26     Oh Pak army or should say porki hijada army wh...      1\n",
      "27     He would be a lone boss with all the supporter...      0\n",
      "28     We want to get rid of u Indians......why don't...      1\n",
      "29     Only 13 % of jio customers converted into prim...      0\n",
      "...                                                  ...    ...\n",
      "11969  It is a good gesture for rewarding to the indi...      0\n",
      "11970  Working on amending AAI Act to monetise AAI's ...      0\n",
      "11971                              Well said Sonu nigam.      0\n",
      "11972  Sarkar fail conges fail bjp fail. Kaha he 1sir...      0\n",
      "11973  Bomb all the porky Army camps in the border......      1\n",
      "11974  how safe is your food ? ....read the article a...      0\n",
      "11975  sc st bc mbc Muslims expect this best solution...      1\n",
      "11976  Dominar, LOL come-on bajaj at least think of a...      0\n",
      "11977  Just dial rallied in the end ..Reason fr so rally      0\n",
      "11978  Rishabh could this headline be any more mislea...      0\n",
      "11979  ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§∏‡§ø‡§∞‡•ç‡§´ ‡§∏‡§§‡•ç‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡•Å‡§Æ‡§≤‡•á‡§¨‡§æ‡§ú‡•Ä ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à ‡§è‡§ï ...      0\n",
      "11980  we don't fear anyone we are not slave of any m...      1\n",
      "11981  Abrar whether you people live in density or al...      1\n",
      "11982  let me come clean.. This is the first time I a...      0\n",
      "11983  DEMONETISATION ==> CONSTITUTIONAL BENCH ‚úî\\n\\n=...      0\n",
      "11984                 What is your further view on ‚Çπvs $      0\n",
      "11985  The biggest explosion of 2017. The big bang ha...      0\n",
      "11986  Indian express did it again ...have some moral...      0\n",
      "11987  Our govt.will not do anything only statement ,...      0\n",
      "11988  Not muslim majority, terrorist majority countries      0\n",
      "11989  Have u ever applied any visa here? I hv been h...      0\n",
      "11990                  Ha ha ha same as Bihar exit polls      0\n",
      "11991  Well....one less educated person in Parliament...      0\n",
      "11992  Akshay Kumar also posted a video of his daught...      0\n",
      "11993                     It's written by me brah... üòÇüòÇüòÇ      0\n",
      "11994  They belong to you flight dirty terrorist coun...      1\n",
      "11995  Really motivating programme, congratulations t...      0\n",
      "11996                                    fabricated news      1\n",
      "11997               What's wrong with you secular idiots      1\n",
      "11998  Looks like inevitable after all political hard...      0\n",
      "\n",
      "[11999 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = df.loc[:,1:2]\n",
    "dataset.columns = ['message', 'class']\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "n8z5XNCd99Vt"
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LG-0Ij2V-Bjl"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1530788057022,
     "user": {
      "displayName": "Akshat Maheshwari",
      "photoUrl": "//lh5.googleusercontent.com/-f-xJkriVoaI/AAAAAAAAAAI/AAAAAAAAAVQ/TLGa4qObGgQ/s50-c-k-no/photo.jpg",
      "userId": "114426356464940466000"
     },
     "user_tz": -330
    },
    "id": "xU-Gzp5X-G0_",
    "outputId": "d4a47aee-ee55-4685-9e09-93f603ab68b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset  (11999, 2)\n",
      "Index(['message', 'class'], dtype='object')\n",
      "No. of unique classes 2\n",
      "NAG:  2708\n",
      "OAG:  9291\n",
      "CAG:  0\n"
     ]
    }
   ],
   "source": [
    "# reading data\n",
    "#df = pd.read_excel('dataset.xlsx')\n",
    "data = dataset.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "print('Shape of dataset ',data.shape)\n",
    "print(data.columns)\n",
    "print('No. of unique classes',len(set(data['class'])))\n",
    "countA=0\n",
    "countB=0\n",
    "countC=0\n",
    "for item in data['class']:\n",
    "    if item == 1:\n",
    "        countA +=1\n",
    "    elif item == 0:\n",
    "        countB +=1\n",
    "    elif item == 'CAG':\n",
    "        countC +=1\n",
    "print(\"NAG: \", countA)\n",
    "print(\"OAG: \",countB)\n",
    "print(\"CAG: \",countC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yTg_bT92-Lmu"
   },
   "outputs": [],
   "source": [
    "macronum=sorted(set(data['class']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "data['class']=data['class'].apply(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1064,
     "status": "ok",
     "timestamp": 1530788064257,
     "user": {
      "displayName": "Akshat Maheshwari",
      "photoUrl": "//lh5.googleusercontent.com/-f-xJkriVoaI/AAAAAAAAAAI/AAAAAAAAAVQ/TLGa4qObGgQ/s50-c-k-no/photo.jpg",
      "userId": "114426356464940466000"
     },
     "user_tz": -330
    },
    "id": "1iuuRu3z-dUl",
    "outputId": "556dd53b-d66d-4a17-f0d6-2c50de94c13f"
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for idx in range(data.message.shape[0]):\n",
    "    text = BeautifulSoup(data.message[idx])\n",
    "    texts.append(clean_str(str(text.get_text().encode())))\n",
    "\n",
    "for idx in data['class']:\n",
    "    labels.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokennization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1410,
     "status": "ok",
     "timestamp": 1530788068659,
     "user": {
      "displayName": "Akshat Maheshwari",
      "photoUrl": "//lh5.googleusercontent.com/-f-xJkriVoaI/AAAAAAAAAAI/AAAAAAAAAVQ/TLGa4qObGgQ/s50-c-k-no/photo.jpg",
      "userId": "114426356464940466000"
     },
     "user_tz": -330
    },
    "id": "UybzQTeT-jGj",
    "outputId": "fee861a5-5673-4b6e-ccd7-f08ccdebf76c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 26361\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1190,
     "status": "ok",
     "timestamp": 1530788073777,
     "user": {
      "displayName": "Akshat Maheshwari",
      "photoUrl": "//lh5.googleusercontent.com/-f-xJkriVoaI/AAAAAAAAAAI/AAAAAAAAAVQ/TLGa4qObGgQ/s50-c-k-no/photo.jpg",
      "userId": "114426356464940466000"
     },
     "user_tz": -330
    },
    "id": "lammuHZ3-puo",
    "outputId": "3ac2e35b-2ab4-48ab-aab8-2253748e9270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data Tensor: (11999, 1000)\n",
      "Shape of Label Tensor: (11999, 2)\n"
     ]
    }
   ],
   "source": [
    "datas = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of Data Tensor:', datas.shape)\n",
    "print('Shape of Label Tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(datas.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "datas = datas[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = datas[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = datas[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12325,
     "status": "ok",
     "timestamp": 1530788092979,
     "user": {
      "displayName": "Akshat Maheshwari",
      "photoUrl": "//lh5.googleusercontent.com/-f-xJkriVoaI/AAAAAAAAAAI/AAAAAAAAAVQ/TLGa4qObGgQ/s50-c-k-no/photo.jpg",
      "userId": "114426356464940466000"
     },
     "user_tz": -330
    },
    "id": "1cia1lKp8YN2",
    "outputId": "ddd4e1b5-a416-484d-8580-86816aeff61d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mvbOk28C-ywX"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1926,
     "status": "ok",
     "timestamp": 1530788208818,
     "user": {
      "displayName": "Akshat Maheshwari",
      "photoUrl": "//lh5.googleusercontent.com/-f-xJkriVoaI/AAAAAAAAAAI/AAAAAAAAAVQ/TLGa4qObGgQ/s50-c-k-no/photo.jpg",
      "userId": "114426356464940466000"
     },
     "user_tz": -330
    },
    "id": "WWAEbYKN-5dL",
    "outputId": "c4bebdf8-16e0-4982-b524-c90ee3f05173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         2636200   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,881,194\n",
      "Trainable params: 2,881,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1074
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63069,
     "status": "ok",
     "timestamp": 1530787445619,
     "user": {
      "displayName": "Akshat Maheshwari",
      "photoUrl": "//lh5.googleusercontent.com/-f-xJkriVoaI/AAAAAAAAAAI/AAAAAAAAAVQ/TLGa4qObGgQ/s50-c-k-no/photo.jpg",
      "userId": "114426356464940466000"
     },
     "user_tz": -330
    },
    "id": "GET42RUmCG1s",
    "outputId": "c85f9471-8081-404f-abfc-528e9dfaceab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2399 samples\n",
      "Epoch 1/15\n",
      "9600/9600 [==============================] - 668s 70ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 2/15\n",
      "9600/9600 [==============================] - 645s 67ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 3/15\n",
      "9600/9600 [==============================] - 652s 68ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 4/15\n",
      "9600/9600 [==============================] - 621s 65ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 5/15\n",
      "9600/9600 [==============================] - 649s 68ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 6/15\n",
      "9600/9600 [==============================] - 641s 67ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 7/15\n",
      "9600/9600 [==============================] - 645s 67ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 8/15\n",
      "9600/9600 [==============================] - 644s 67ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 9/15\n",
      "9600/9600 [==============================] - 624s 65ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 10/15\n",
      "9600/9600 [==============================] - 624s 65ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 11/15\n",
      "9600/9600 [==============================] - 624s 65ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 12/15\n",
      "9600/9600 [==============================] - 652s 68ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 13/15\n",
      "9600/9600 [==============================] - 654s 68ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 14/15\n",
      "9600/9600 [==============================] - 675s 70ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 15/15\n",
      "8730/9600 [==========================>...] - ETA: 56s - loss: 3.5935 - f1: 0.7758"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=15, batch_size=2,callbacks=[cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2399 samples\n",
      "Epoch 1/2\n",
      "9600/9600 [==============================] - 652s 68ms/step - loss: 3.5905 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n",
      "Epoch 2/2\n",
      "   2/9600 [..............................] - ETA: 10:25 - loss: 1.0960e-07 - f1: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirafra/anaconda3/envs/abhishek/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600/9600 [==============================] - 623s 65ms/step - loss: 3.5918 - f1: 0.7759 - val_loss: 3.7219 - val_f1: 0.7678\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=2, batch_size=2,callbacks=[cp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using the saved model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#score = loaded_model.evaluate(x_val, y_val, verbose=0)\n",
    "#print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make prediction using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = loaded_model.predict()\n",
    "labels = np.argmax(probas, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAG', 'NAG', 'OAG']\n",
      "[2 2 1 0]\n",
      "['OAG', 'OAG', 'NAG', 'CAG']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"NAG\",\"OAG\",\"CAG\"])\n",
    "print(list(le.classes_))\n",
    "print(le.transform([\"OAG\", \"OAG\", \"NAG\",\"CAG\"]))\n",
    "print(list(le.inverse_transform([2, 2, 1,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction using saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75133294 0.24866706]\n",
      " [0.8484862  0.15151377]\n",
      " [0.49108267 0.5089173 ]\n",
      " ...\n",
      " [0.92430913 0.07569086]\n",
      " [0.5553795  0.4446204 ]\n",
      " [0.80169934 0.19830066]]\n",
      "[0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0\n",
      " 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "pred = loaded_model.predict(datas_new)\n",
    "#accuracy_score(y_val, pred)\n",
    "labels = np.argmax(pred, axis=-1)\n",
    "print(pred)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...\n",
      "1      #ConstitutionDay is revered by Conservatives, ...\n",
      "2      #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...\n",
      "3      #Watching #Boomer getting the news that she is...\n",
      "4      #NoPasaran: Unity demo to oppose the far-right...\n",
      "5               . . . What the fuck did he do this time?\n",
      "6      #RAP is a form of ART! Used to express yoursel...\n",
      "7      @USER Do you get the feeling he is kissing @US...\n",
      "8      5 Tips to Enhance Audience Connection on Faceb...\n",
      "9      #BiggBossTamil janani won the task. She is goi...\n",
      "10     #Conservatives - the party of low taxation üòÇ #...\n",
      "11     ùìí-ùìíùìÆùì≠ùìªùì™ùì∑..  The Nord cannot make a single move...\n",
      "12       #ConsTOO THE PLACE FOR FED UP CONSERVATIVES !!!\n",
      "13     #GreatAwakening #QAnon #PatriotsUnited #WWG1WG...\n",
      "14                           @USER Nigga ware da hits at\n",
      "15     #StopKavanaugh he is liar like the rest of the...\n",
      "16     #Dayspromo week of September 17th #days #dool ...\n",
      "17     #BiggBossTelugu2 Let‚Äôs all include @USER in al...\n",
      "18     #Dallas#Policewoman kills neighbor in HIS own ...\n",
      "19                                      Always smack URL\n",
      "20     #BREAKING: After a week-long trial, a Linn Cou...\n",
      "21     #Kavanaugh so a wild claim from 36 years ago o...\n",
      "22     #kznlt Enjouji really is the prototype of my c...\n",
      "23     #SilsilaBadallteRishtonKa tag is filled with a...\n",
      "24     #Beto #BetoForSenate #BetoORourke #BetoforTX  ...\n",
      "25     #HIAC  Damn Matt Hardy and Randy Orton put on ...\n",
      "26     @USER School shootings aren't controversial. W...\n",
      "27     @USER @USER Put DeLauro in a police lineup ide...\n",
      "28     @USER *coffee spit* Spartacus moment?  He neve...\n",
      "29     @ ALL FAMILY/FRIENDS , do not tell me  bad shi...\n",
      "                             ...                        \n",
      "830    #ARSYoungjaeDay HAPPY BIRTHDAY CHOI YOUNGJAE üò≠...\n",
      "831    ? Tickets for #htafc's away match against #Wat...\n",
      "832    @USER u put a privileged NFL player in ur ad f...\n",
      "833    #ConfirmKavanugh now, stall tactics are DC cro...\n",
      "834    @USER @USER @USER @USER Beats the hell NJ out ...\n",
      "835    #StopEtchecopar? Fuck you all üñïüñïüñïüñïüñï Que florez...\n",
      "836    #Antifa invaded a memorial in recognition of K...\n",
      "837    @USER Laws are for the law abiding citizens. W...\n",
      "838    #OnAirNow Jon Bellion - He Is The Same URL #1R...\n",
      "839    #Immigration Public Charge Rule has been law f...\n",
      "840    #Florence List:  URL  Hurricane Florence feeds...\n",
      "841    #Feckless I encourage liberals and Democrats n...\n",
      "842    #Antifa are mentally unstable cowards, pretend...\n",
      "843    #Emmys2018 Sanda Ho is not Asia America she is...\n",
      "844    #WeLoveSergioBecause he is the sweetest guy ev...\n",
      "845    @USER @USER I'm in NC. Thursday's I work at a ...\n",
      "846    I'am so sleepy.. { Bot cuddles up to  you }  {...\n",
      "847    @USER @USER And Browning looked like dog shit ...\n",
      "848    #WCW - @USER does it again with her live cover...\n",
      "849    #CTRiders Hope you are enjoying your Friday! I...\n",
      "850    1) Wow, safe sex! That's hot! We love a butch ...\n",
      "851    ...ANTIFA men\" get themselves riled up before ...\n",
      "852    #BrettKavanaugh   ... but anyway, there is the...\n",
      "853                  All two of them taste like ass. URL\n",
      "854    #TEAM4THEBEEZ PLEASE SHARE THIS PUP DID NOTHIN...\n",
      "855    #DespicableDems lie again about rifles. Dem Di...\n",
      "856    #MeetTheSpeakers üôå @USER will present in our e...\n",
      "857    3 people just unfollowed me for talking about ...\n",
      "858    #WednesdayWisdom Antifa calls the right fascis...\n",
      "859        #Kavanaugh typical #liberals , #Democrats URL\n",
      "Name: tweet, Length: 860, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#df_test = pd.read_csv('/home/mirafra/Desktop/nlp_project/sample_output/Test A Release/testset-taska.csv')\n",
    "df_test = pd.read_csv('/home/mirafra/Desktop/nlp_project/english/agr_en_test.csv',delimiter='\\t',encoding='utf-8')\n",
    "x_val = df_test['tweet']\n",
    "print(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                message class\n",
      "0     The quality of re made now makes me think it i...   CAG\n",
      "1     @siva \\nHow is ur mother???\\nHow is ur wife???...   NAG\n",
      "2     Also see ....hw ur RSS activist caught in Burk...   NAG\n",
      "3     On the death of 2 jawans in LOC CROSS FIRING\\n...   NAG\n",
      "4     Modi ho ya Manmohan singh saala yeh log kuch n...   OAG\n",
      "5     No discrimination in policies and NON protecti...   CAG\n",
      "6     As Mr Ashwini gujral said once nifty will touc...   NAG\n",
      "7     After seeing your comment guys.. I think sonu ...   CAG\n",
      "8     What has so far Mr.Yechuri done for this Count...   CAG\n",
      "9     SONIA SHENOY.....A KONKONI.....WRONGLY INTERPR...   CAG\n",
      "10              Now all AAPtards will abuse ANNA HAJARE   NAG\n",
      "11    I salute ..@ Neel Patel,,U r just amazing. Eac...   NAG\n",
      "12    Hi Sonia!! Markets & reliance zooming like roc...   NAG\n",
      "13    These kashmiri youth are radicalized jihadi te...   OAG\n",
      "14    If you want to construct a smooth road you hav...   CAG\n",
      "15    Phantom 2 under production where Saif Ali Khan...   CAG\n",
      "16    Nothing can change your attitude towards musli...   CAG\n",
      "17    Even i didn't watch dangal till now but this m...   CAG\n",
      "18    Cheap publicity...can he also answer why shoul...   OAG\n",
      "19                                                  Yes   NAG\n",
      "20                       Puppies are way more important   CAG\n",
      "21    Says a guy who lives in a Christian country Au...   OAG\n",
      "22                                            BJP Agent   OAG\n",
      "23    Ambuja cements results are very bad... but y i...   OAG\n",
      "24    He is right bjp has all kind of people in part...   NAG\n",
      "25                      Is there any end to bother pill   CAG\n",
      "26    The live feed is getting disconnected again an...   NAG\n",
      "27    Anuj sir y bank nifty not stopping?now.need to...   NAG\n",
      "28    Muni I have no proof. But I am a common man an...   NAG\n",
      "29                          How many people died on dat   NAG\n",
      "...                                                 ...   ...\n",
      "2971  repeating  the news 2011 , 2012 , 2013 , 2014 ...   NAG\n",
      "2972  What impact will it have on the stock price of...   NAG\n",
      "2973  Ashish Mullick India Did Sushma tell the milli...   CAG\n",
      "2974  i am micro investor, need advise on adding sto...   NAG\n",
      "2975  I am not able to understand why our government...   OAG\n",
      "2976  Infact India is worrying about the fate of Ker...   NAG\n",
      "2977    Not out means will we c som high in near future   NAG\n",
      "2978  He is trying for his foundation, which have be...   NAG\n",
      "2979  why can't we gave the same reply to them .Use ...   NAG\n",
      "2980  Are we not used to stand in queues??? And afte...   CAG\n",
      "2981                   Sanjeev Singh is behind all this   NAG\n",
      "2982  I am clean without cash as transactions are th...   NAG\n",
      "2983  Everyone is in the race of earning money there...   OAG\n",
      "2984  Only sacked...she shud be in jail for a long l...   OAG\n",
      "2985  Needs courage. But time has come to show this ...   CAG\n",
      "2986                    Teri maa ki chuuut kutte chuude   OAG\n",
      "2987                                        He is right   CAG\n",
      "2988                    Unexpected from u mr Sonu nigam   CAG\n",
      "2989  It is a non-story. You should not have filed i...   OAG\n",
      "2990  Another 100 in the name of husband as an added...   NAG\n",
      "2991                                          Precisely   NAG\n",
      "2992  Truth.... It's all about politics... I think m...   NAG\n",
      "2993                        You r at liberty to analyse   NAG\n",
      "2994  Great stand by father but the guy needs a buri...   CAG\n",
      "2995  PAYTM is a unscrupulous vendor see my experien...   CAG\n",
      "2996                     Gaddaron se to yahi sahi hain.   NAG\n",
      "2997  Just saw this news\\n* read 1dt comment. \\n*smi...   CAG\n",
      "2998  U guys can crack jokes but can't take the crid...   CAG\n",
      "2999  These media fellows have time to dissect one's...   CAG\n",
      "3000  Middle class is suffering, but most affected a...   NAG\n",
      "\n",
      "[3001 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_test_new = df_test.loc[:,1:2]\n",
    "df_test_new.columns = ['message', 'class']\n",
    "print(df_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset  (860,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for item in df_test_new[\\'class\\']:\\n    if item == \\'NAG\\':\\n        countA +=1\\n    elif item == \\'OAG\\':\\n        countB +=1\\n    elif item == \\'CAG\\':\\n        countC +=1\\nprint(\"NAG: \", countA)\\nprint(\"OAG: \",countB)\\nprint(\"CAG: \",countC)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data\n",
    "#df = pd.read_excel('dataset.xlsx')\n",
    "df_test_new = x_val.dropna()\n",
    "df_test_new = df_test_new.reset_index(drop=True)\n",
    "print('Shape of dataset ',df_test_new.shape)\n",
    "#print(df_test_new.columns)\n",
    "#print('No. of unique classes',len(set(df_test_new['class'])))\n",
    "countA=0\n",
    "countB=0\n",
    "countC=0\n",
    "'''for item in df_test_new['class']:\n",
    "    if item == 'NAG':\n",
    "        countA +=1\n",
    "    elif item == 'OAG':\n",
    "        countB +=1\n",
    "    elif item == 'CAG':\n",
    "        countC +=1\n",
    "print(\"NAG: \", countA)\n",
    "print(\"OAG: \",countB)\n",
    "print(\"CAG: \",countC)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "macronum=sorted(set(df_test_new['class']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "df_test_new['class']=df_test_new['class'].apply(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new = []\n",
    "labels_new = []\n",
    "\n",
    "\n",
    "for idx in range(df_test_new.shape[0]):\n",
    "    text = BeautifulSoup(df_test_new[idx])\n",
    "    texts_new.append(clean_str(str(text.get_text().encode())))\n",
    "\n",
    "#for idx in df_test_new['class']:\n",
    "#    labels_new.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 5463\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_new)\n",
    "sequences = tokenizer.texts_to_sequences(texts_new)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data Tensor: (860, 1000)\n"
     ]
    }
   ],
   "source": [
    "datas_new = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#labels_new = to_categorical(np.asarray(labels_new))\n",
    "print('Shape of Data Tensor:', datas_new.shape)\n",
    "#print('Shape of Label Tensor:', labels_new.shape)\n",
    "\n",
    "indices = np.arange(datas_new.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "datas_new = datas_new[indices]\n",
    "#labels_new = labels_new[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75133294 0.24866706]\n",
      " [0.8484862  0.15151377]\n",
      " [0.49108267 0.5089173 ]\n",
      " ...\n",
      " [0.92430913 0.07569086]\n",
      " [0.5553795  0.4446204 ]\n",
      " [0.80169934 0.19830066]]\n",
      "[0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0\n",
      " 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "pred = loaded_model.predict(datas_new)\n",
    "#accuracy_score(y_val, pred)\n",
    "labels = np.argmax(pred, axis=-1)\n",
    "print(pred)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Result = {'id':df_test['id'],'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Test_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>65507</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>78910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46363</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>68123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64376</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>65773</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>95457</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24930</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>45712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>70840</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>53563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>59432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>21454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>83155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>69576</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>49139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>76669</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>58995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>80406</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>73366</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>26454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>78688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>76135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>30778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>48418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>10313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>78950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>13993</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>32492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>50781</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>22569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>87317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>53862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>89424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>22470</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>48938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>85360</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>90032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>31182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>83464</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>51218</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>41438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>72867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>73439</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>25657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>67018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>50665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>24583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  labels\n",
       "0    15923       0\n",
       "1    27014       0\n",
       "2    30530       1\n",
       "3    13876       0\n",
       "4    60133       1\n",
       "5    83681       0\n",
       "6    96874       0\n",
       "7    65507       0\n",
       "8    78910       0\n",
       "9    46363       1\n",
       "10   68123       0\n",
       "11   22452       0\n",
       "12   15565       0\n",
       "13   64376       0\n",
       "14   12588       0\n",
       "15   34263       0\n",
       "16   65773       0\n",
       "17   95457       1\n",
       "18   24930       0\n",
       "19   15938       0\n",
       "20   45712       0\n",
       "21   70840       0\n",
       "22   53563       0\n",
       "23   59432       0\n",
       "24   21454       0\n",
       "25   83155       1\n",
       "26   69576       1\n",
       "27   49139       0\n",
       "28   76669       0\n",
       "29   58995       0\n",
       "..     ...     ...\n",
       "830  80406       0\n",
       "831  73366       0\n",
       "832  26454       0\n",
       "833  78688       1\n",
       "834  76135       0\n",
       "835  30778       1\n",
       "836  48418       0\n",
       "837  10313       0\n",
       "838  78950       0\n",
       "839  13993       0\n",
       "840  32492       1\n",
       "841  50781       0\n",
       "842  22569       1\n",
       "843  87317       1\n",
       "844  53862       0\n",
       "845  89424       1\n",
       "846  22470       0\n",
       "847  48938       1\n",
       "848  85360       0\n",
       "849  90032       0\n",
       "850  31182       0\n",
       "851  83464       1\n",
       "852  51218       0\n",
       "853  41438       0\n",
       "854  72867       0\n",
       "855  73439       0\n",
       "856  25657       0\n",
       "857  67018       0\n",
       "858  50665       0\n",
       "859  24583       0\n",
       "\n",
       "[860 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result_task_A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "CNN.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
